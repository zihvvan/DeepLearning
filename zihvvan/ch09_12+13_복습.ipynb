{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zihvvan/DeepLearning/blob/main/zihvvan/ch09_12%2B13_%EB%B3%B5%EC%8A%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM & Seq2Seq & Transformer"
      ],
      "metadata": {
        "id": "PopnM6LlEzJm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Course"
      ],
      "metadata": {
        "id": "aT523pfcE4et"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM"
      ],
      "metadata": {
        "id": "0ObUwn8GO8EM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 딥러닝 신경망에서 LSTM이 뭐야? RNN과의 차이점을 중심으로 설명해줘. PyTorch 코드 예시와 함께\n",
        "* 장기 의존성, 입력 시퀀스, 기울기 소실\n",
        "* 게이트(Gate) 메커니즘, 망각 게이트, 입력 게이트, 출력 게이트\n",
        "* 셀(Cell) 상태, 은닉 상태(Hidden state)\n",
        "* https://sharegpt.com/c/M3iL1gM\n",
        "\n",
        "> LSTM의 구조와 원리를 구체적인 비유와 실생활의 개념을 바탕으로 설명해줘\n",
        "* https://shareg.pt/4XF0dza"
      ],
      "metadata": {
        "id": "CT7cI3lNFfVl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> LSTM을 통한 자연어 처리 및 문제 해결의 예시를 3가지 들어줘\n",
        "* https://sharegpt.com/c/QATjAWT\n",
        "\n",
        "> LSTM을 통해서 비디오 분석 또는 오디오 분석을 했을 때의 예제와 그 코드 및 데이터를 3개 제시해줘\n",
        "* https://sharegpt.com/c/GpbjF9q"
      ],
      "metadata": {
        "id": "A0tQEGGAHhJL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> PyTorch 딥러닝에서 LSTM을 통해서 문장 생성 프로젝트를 진행할 때, 한국어 자연어 전처리 및 후처리에 대한 필요한 내용들에 대해서 구체적으로 설명해줘\n",
        "* https://sharegpt.com/c/Ng1VUyP"
      ],
      "metadata": {
        "id": "Wf8b0SnNI8dA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Seq2Seq & 어텐션 메커니즘"
      ],
      "metadata": {
        "id": "vbeYJrAUOtgT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 딥러닝에서 Seq2Seq를 LSTM의 한계와 인코더, 디코더에 대한 내용을 포함해서 설명해줘\n",
        "* LSTM, 인코더, 디코더\n",
        "* https://sharegpt.com/c/o0w5IeP"
      ],
      "metadata": {
        "id": "GK9f_nI0O_EI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 딥러닝에서의 Seq2Seq의 구조와 원리를, 실생활적인 비유를 통해서 설명해줘\n",
        "* https://sharegpt.com/c/9RtedqX"
      ],
      "metadata": {
        "id": "Jrc2vvkvP68F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 딥러닝에서의 Seq2Seq를 LSTM과의 공통점과 차이점, 한계 등과 함께 설명해줘. 그리고 구체적인 구조의 차이를 파이썬 코드 예제와 함께 제시해줘.\n",
        "* https://sharegpt.com/c/PleORcA"
      ],
      "metadata": {
        "id": "b_eIDCvBQJYu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 딥러닝에서 어텐션 메커니즘을 Seq2Seq의 한계와 어텐션 가중치에 대한 내용을 포함해서 설명해줘\n",
        "* https://shareg.pt/EelC3Xt\n",
        "\n",
        "> 딥러닝 어텐션 메커니즘을 Seq2Seq와의 차이점을 중심으로, 실생활적인 비유를 들어서 설명해줘\n",
        "* https://sharegpt.com/c/sX4cl3A"
      ],
      "metadata": {
        "id": "vqf81KqRSN7K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer"
      ],
      "metadata": {
        "id": "nkj_uAJ8TqYA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 딥러닝에서, Transformer의 개념을, LSTM, seq2seq, 어텐션 메커니즘으로 발전해온 흐름에 비춰서 설명해줘\n",
        "* https://sharegpt.com/c/5ipHe6n\n",
        "\n",
        "> 딥러닝에서 Transfomer의 구조와 기능을 셀프 어텐션 메커니즘, 멀티 헤드 셀프 어텐션, 쿼리-키-밸류 구조를 바탕으로 실생활 비유를 들어서 설명해줘\n",
        "* https://shareg.pt/0ShBpQK"
      ],
      "metadata": {
        "id": "OQEsOPHgUReQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 딥러닝 Transformer에서 내적의 역할과, 내적 자체의 개념을 자세히 비유와 함께 설명해줘\n",
        "* https://sharegpt.com/c/eR23FAt"
      ],
      "metadata": {
        "id": "md9j6AOfVUEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 딥러닝에서 Transformer가 어느 분야에서 활용되는지와 최신 Transtormer 알고리즘들의 예시를 알려줘\n",
        "* https://sharegpt.com/c/G251M5y"
      ],
      "metadata": {
        "id": "GlDtNxvbV4hX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **HuggingFace**를 통해 Transformer 모델을 찾는 방법과, PyTorch로 해당 모델을 전이학습하는 방법에 대해서 예시코드와 함께 설명해줘.\n",
        "* https://sharegpt.com/c/MjWuQOQ\n",
        "* https://huggingface.co/"
      ],
      "metadata": {
        "id": "UvM_lBPbWcuI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **PORORO** (카카오브레인 만든 자연어 처리 패키지)\n",
        "* https://smilegate.ai/2021/02/10/kakaobrain-pororo/\n",
        "* https://teddylee777.github.io/machine-learning/nlp-korean-pororo/"
      ],
      "metadata": {
        "id": "bStpuMJSXjW2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Challenge"
      ],
      "metadata": {
        "id": "cw3uuTj9E57P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 빈 칸 채우기\n",
        "* 연습문제\n",
        "* 에러 코드 (TDD)"
      ],
      "metadata": {
        "id": "gdVYnm-gXqcO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creativity"
      ],
      "metadata": {
        "id": "Z6edUbAUE7le"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 간단한 복습용 데이터 프로젝트\n",
        "* 본인 직무와 관련된 심화 프로젝트 & 팀 프로젝트\n",
        "* 결과물들을 어떻게 문서화 & 시각화"
      ],
      "metadata": {
        "id": "gsQppvLrXtbp"
      }
    }
  ]
}